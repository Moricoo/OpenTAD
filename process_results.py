# -*- coding: utf-8 -*-
"""
å¤„ç†æ¨ç†ç»“æœï¼Œå°†åŸå§‹è¾“å‡ºè½¬æ¢ä¸ºå¯è¯»çš„åŠ¨ä½œæ£€æµ‹ç»“æœ
"""
import os
import sys
sys.path.insert(0, "/root/OpenTAD")
os.chdir("/root/OpenTAD")

import torch
import json
import numpy as np
from mmengine.config import Config
from opentad.models import build_detector
from opentad.models.utils.post_processing import batched_nms, convert_to_seconds

def process_inference_results():
    """å¤„ç†æ¨ç†ç»“æœï¼Œè¿›è¡Œåå¤„ç†"""
    
    # åŠ è½½é…ç½®
    config_path = "model_package_thumos_adapter_baseline/config/e2e_thumos_videomae_s_768x1_160_adapter.py"
    checkpoint_path = "model_package_thumos_adapter_baseline/checkpoint/latest.pth"
    
    print("=" * 70)
    print("å¤„ç†æ¨ç†ç»“æœ - åå¤„ç†å’Œæ ¼å¼åŒ–")
    print("=" * 70)
    
    # åŠ è½½é…ç½®
    print("\n[1/4] åŠ è½½é…ç½®...")
    cfg = Config.fromfile(config_path)
    
    # æ„å»ºæ¨¡å‹ï¼ˆä»…ç”¨äºåå¤„ç†ï¼‰
    print("[2/4] æ„å»ºæ¨¡å‹...")
    model = build_detector(cfg.model)
    checkpoint = torch.load(checkpoint_path, map_location="cpu")
    if "state_dict" in checkpoint:
        model.load_state_dict(checkpoint["state_dict"], strict=False)
    model.eval()
    print("  âœ… æ¨¡å‹åŠ è½½å®Œæˆ")
    
    # è¯»å–ç±»åˆ«æ˜ å°„
    print("[3/4] è¯»å–ç±»åˆ«æ˜ å°„...")
    with open("inference_category_idx.txt", "r", encoding="utf-8") as f:
        class_names = [line.strip() for line in f.readlines()]
    print(f"  âœ… å…± {len(class_names)} ä¸ªç±»åˆ«")
    
    # è¯»å–åŸå§‹ç»“æœ
    print("[4/4] å¤„ç†æ¨ç†ç»“æœ...")
    with open("inference_results.json", "r", encoding="utf-8") as f:
        raw_data = json.load(f)
    
    video_name = raw_data["video"]
    raw_results = raw_data["results"]
    
    print(f"\nå¤„ç†è§†é¢‘: {video_name}")
    print(f"åŸå§‹ç»“æœæ•°é‡: {len(raw_results)}")
    
    # è¯»å– annotation è·å–è§†é¢‘å…ƒä¿¡æ¯
    with open("inference_annotation.json", "r", encoding="utf-8") as f:
        ann_data = json.load(f)
    
    video_key = list(ann_data["database"].keys())[0]
    video_info = ann_data["database"][video_key]
    fps = video_info["frame"] / video_info["duration"]
    
    # å¤„ç†æ¯ä¸ªçª—å£çš„ç»“æœ
    all_detections = []
    
    for window_idx, window_result in enumerate(raw_results):
        print(f"\nå¤„ç†çª—å£ {window_idx + 1}/{len(raw_results)}...")
        
        # window_result åº”è¯¥æ˜¯æ¨¡å‹çš„åŸå§‹è¾“å‡º
        # æ ¹æ® ActionFormer çš„è¾“å‡ºæ ¼å¼ï¼Œåº”è¯¥æ˜¯ [segments, scores, labels] æˆ–ç±»ä¼¼æ ¼å¼
        # è¿™é‡Œéœ€è¦æ ¹æ®å®é™…è¾“å‡ºæ ¼å¼è°ƒæ•´
        
        if isinstance(window_result, list) and len(window_result) > 0:
            # å°è¯•è§£æç»“æœ
            # é€šå¸¸ ActionFormer è¾“å‡ºæ˜¯é¢„æµ‹çš„ segments å’Œ scores
            print(f"  ç»“æœç±»å‹: {type(window_result)}")
            print(f"  ç»“æœé•¿åº¦: {len(window_result)}")
            
            # å¦‚æœç»“æœåŒ…å«å¤šä¸ªåˆ—è¡¨ï¼Œå¯èƒ½æ˜¯ [segments, scores, labels]
            if isinstance(window_result[0], list):
                if len(window_result) >= 2:
                    # å‡è®¾æ˜¯ segments å’Œ scores
                    segments = torch.tensor(window_result[0]) if isinstance(window_result[0][0], (int, float)) else None
                    scores = torch.tensor(window_result[1]) if len(window_result) > 1 and isinstance(window_result[1][0], (int, float)) else None
                    
                    if segments is not None and scores is not None:
                        print(f"  æ£€æµ‹åˆ° {len(segments)} ä¸ªå€™é€‰ç‰‡æ®µ")
                        
                        # åº”ç”¨ NMS
                        if len(segments.shape) == 2 and segments.shape[1] == 2:
                            # segments æ ¼å¼: [N, 2] (start, end)
                            labels = torch.zeros(len(segments), dtype=torch.long)  # ä¸´æ—¶æ ‡ç­¾
                            
                            # NMS åå¤„ç†
                            nms_config = cfg.post_processing.nms
                            segments_nms, scores_nms, labels_nms = batched_nms(
                                segments.unsqueeze(0),
                                scores.unsqueeze(0),
                                labels.unsqueeze(0),
                                **nms_config
                            )
                            
                            segments_nms = segments_nms[0]
                            scores_nms = scores_nms[0]
                            labels_nms = labels_nms[0]
                            
                            # è½¬æ¢ä¸ºç§’æ•°
                            meta = {
                                "video_name": video_key,
                                "fps": fps,
                                "duration": video_info["duration"],
                                "frame": video_info["frame"]
                            }
                            
                            segments_seconds = convert_to_seconds(segments_nms, meta)
                            
                            # æ ¼å¼åŒ–ç»“æœ
                            for seg, score, label_idx in zip(segments_seconds, scores_nms, labels_nms):
                                label_name = class_names[label_idx.item()] if label_idx.item() < len(class_names) else f"Class_{label_idx.item()}"
                                all_detections.append({
                                    "window": window_idx + 1,
                                    "segment": [round(seg[0].item(), 2), round(seg[1].item(), 2)],
                                    "label": label_name,
                                    "score": round(score.item(), 4),
                                    "start_time": f"{int(seg[0].item())//60:02d}:{int(seg[0].item())%60:02d}",
                                    "end_time": f"{int(seg[1].item())//60:02d}:{int(seg[1].item())%60:02d}",
                                })
    
    # ä¿å­˜å¤„ç†åçš„ç»“æœ
    output_data = {
        "video": video_name,
        "duration": video_info["duration"],
        "fps": fps,
        "total_frames": video_info["frame"],
        "detections": all_detections,
        "summary": {
            "total_detections": len(all_detections),
            "windows_processed": len(raw_results)
        }
    }
    
    output_path = "inference_results_processed.json"
    with open(output_path, "w", encoding="utf-8") as f:
        json.dump(output_data, f, indent=2, ensure_ascii=False)
    
    print("\n" + "=" * 70)
    print("âœ… ç»“æœå¤„ç†å®Œæˆï¼")
    print(f"ğŸ“Š æ£€æµ‹åˆ° {len(all_detections)} ä¸ªåŠ¨ä½œç‰‡æ®µ")
    print(f"ğŸ“ å¤„ç†åçš„ç»“æœå·²ä¿å­˜åˆ°: {output_path}")
    print("=" * 70)
    
    # æ‰“å°å‰å‡ ä¸ªæ£€æµ‹ç»“æœ
    if all_detections:
        print("\nå‰ 10 ä¸ªæ£€æµ‹ç»“æœ:")
        print("-" * 70)
        for i, det in enumerate(all_detections[:10]):
            print(f"{i+1}. {det['label']} | "
                  f"æ—¶é—´: {det['start_time']} - {det['end_time']} "
                  f"({det['segment'][0]:.1f}s - {det['segment'][1]:.1f}s) | "
                  f"ç½®ä¿¡åº¦: {det['score']:.4f}")
    else:
        print("\nâš ï¸  æœªæ£€æµ‹åˆ°ä»»ä½•åŠ¨ä½œç‰‡æ®µ")
        print("è¿™å¯èƒ½æ˜¯å› ä¸º:")
        print("  1. æ¨¡å‹è¾“å‡ºæ ¼å¼éœ€è¦è¿›ä¸€æ­¥è§£æ")
        print("  2. éœ€è¦è°ƒæ•´ç½®ä¿¡åº¦é˜ˆå€¼")
        print("  3. åŸå§‹ç»“æœéœ€è¦ä¸åŒçš„åå¤„ç†æ–¹å¼")

if __name__ == "__main__":
    process_inference_results()

