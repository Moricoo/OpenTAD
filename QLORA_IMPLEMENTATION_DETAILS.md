# QLoRAä¸AdaTADç»“åˆå®ç°è¯¦è§£

## ğŸ“‹ ç›®å½•
1. [æ•´ä½“æ¶æ„æ¦‚è§ˆ](#æ•´ä½“æ¶æ„æ¦‚è§ˆ)
2. [æ ¸å¿ƒç»„ä»¶å®ç°](#æ ¸å¿ƒç»„ä»¶å®ç°)
3. [ç½‘ç»œç»“æ„è¯¦è§£](#ç½‘ç»œç»“æ„è¯¦è§£)
4. [é‡åŒ–ä¸LoRAæœºåˆ¶](#é‡åŒ–ä¸loraæœºåˆ¶)
5. [è®­ç»ƒç­–ç•¥](#è®­ç»ƒç­–ç•¥)

---

## ğŸ—ï¸ æ•´ä½“æ¶æ„æ¦‚è§ˆ

### 1. ç³»ç»Ÿå±‚æ¬¡ç»“æ„

```
AdaTAD (ActionFormer)
    â””â”€â”€ Backbone: Recognizer3D
        â””â”€â”€ Vision Transformer (VideoMAE-S)
            â””â”€â”€ 12ä¸ª Transformer Blocks
                â””â”€â”€ æ¯ä¸ªBlockåŒ…å«:
                    â”œâ”€â”€ Self-Attention (å†»ç»“)
                    â”œâ”€â”€ MLP (å†»ç»“)
                    â””â”€â”€ QLoRA Adapter (å¯è®­ç»ƒ) â­
```

### 2. QLoRA Adapteråœ¨Blockä¸­çš„ä½ç½®

```
VisionTransformer Blockç»“æ„:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Input: x [B, N, C]                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  1. Norm1                            â”‚
â”‚  2. Self-Attention (å†»ç»“)            â”‚
â”‚     x = x + DropPath(Attn(Norm1(x))) â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  3. Norm2                            â”‚
â”‚  4. MLP (å†»ç»“)                       â”‚
â”‚     x = x + DropPath(MLP(Norm2(x))) â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  5. QLoRA Adapter (å¯è®­ç»ƒ) â­        â”‚
â”‚     x = Adapter(x, h, w)             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ”§ æ ¸å¿ƒç»„ä»¶å®ç°

### 1. QLoRAAdapterç±» (å®Œæ•´ç‰ˆï¼ŒåŒ…å«Temporal Convolution)

```python
class QLoRAAdapter(BaseModule):
    """
    æ ¸å¿ƒæ€æƒ³ï¼š
    1. ä½¿ç”¨4-bité‡åŒ–Linearå±‚æ›¿ä»£åŸå§‹Linearå±‚ï¼ˆå‡å°‘æ˜¾å­˜ï¼‰
    2. æ·»åŠ LoRAå±‚è¿›è¡Œå‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆå‡å°‘å‚æ•°é‡ï¼‰
    3. ä¿ç•™Temporal Convolutionï¼ˆä¿æŒæ—¶åºå»ºæ¨¡èƒ½åŠ›ï¼‰
    """

    def __init__(
        self,
        embed_dims: int = 384,      # åµŒå…¥ç»´åº¦
        mlp_ratio: float = 0.25,     # Adapterçš„éšè—å±‚æ¯”ä¾‹
        lora_r: int = 16,            # LoRA rank
        lora_alpha: int = 32,        # LoRAç¼©æ”¾å› å­
        quantize_bits: int = 4,      # é‡åŒ–ä½æ•°
        temporal_size: int = 384,    # æ—¶åºé•¿åº¦
    ):
        # 1. é‡åŒ–Linearå±‚ï¼ˆæ›¿ä»£åŸå§‹Linearï¼‰
        self.quantize_fn = bnb.nn.Linear4bit if quantize_bits == 4 else bnb.nn.Linear8bitLt
        self.down_proj = self.quantize_fn(embed_dims, hidden_dims, bias=False)
        self.up_proj = self.quantize_fn(hidden_dims, embed_dims, bias=False)

        # 2. LoRAå±‚ï¼ˆä½ç§©åˆ†è§£ï¼‰
        # Down Projectionçš„LoRA
        self.down_lora_a = nn.Linear(embed_dims, lora_r, bias=False)      # [384, 16]
        self.down_lora_b = nn.Linear(lora_r, hidden_dims, bias=False)     # [16, 96]

        # Up Projectionçš„LoRA
        self.up_lora_a = nn.Linear(hidden_dims, lora_r, bias=False)      # [96, 16]
        self.up_lora_b = nn.Linear(lora_r, embed_dims, bias=False)       # [16, 384]

        # 3. Temporal Convolutionï¼ˆä¿æŒåŸæ ·ï¼Œä¸é‡åŒ–ï¼‰
        self.dwconv = nn.Conv1d(hidden_dims, hidden_dims, kernel_size=3, ...)
        self.conv = nn.Conv1d(hidden_dims, hidden_dims, 1)

        # 4. ç¼©æ”¾å› å­
        self.gamma = nn.Parameter(torch.ones(1))

    def forward(self, x: Tensor, h: int, w: int) -> Tensor:
        inputs = x  # æ®‹å·®è¿æ¥

        # === Down Projection: é‡åŒ– + LoRA ===
        x_quantized = self.down_proj(x)                    # é‡åŒ–Linear: [B, N, 384] -> [B, N, 96]
        x_lora = self.down_lora_b(                         # LoRAè·¯å¾„
            self.down_lora_a(x)                            # [B, N, 384] -> [B, N, 16]
        ) * (self.lora_alpha / self.lora_r)               # [B, N, 16] -> [B, N, 96] * (32/16)
        x = x_quantized + x_lora                          # é‡åŒ–ç»“æœ + LoRAç»“æœ
        x = self.act(x)                                    # GELUæ¿€æ´»

        # === Temporal Convolution ===
        # é‡å¡‘ä¸ºæ—¶ç©ºæ ¼å¼: [B, N, C] -> [B, T, H, W, C]
        B, N, C = x.shape
        attn = x.reshape(-1, self.temporal_size, h, w, C)
        attn = attn.permute(0, 2, 3, 4, 1).flatten(0, 2)  # [B*H*W, C, T]
        attn = self.dwconv(attn)                          # æ·±åº¦å¯åˆ†ç¦»å·ç§¯
        attn = self.conv(attn)                            # 1x1å·ç§¯
        attn = attn.unflatten(0, (-1, h, w)).permute(0, 4, 1, 2, 3)
        attn = attn.reshape(B, N, C)
        x = x + attn                                      # æ®‹å·®è¿æ¥

        # === Up Projection: é‡åŒ– + LoRA ===
        x_quantized = self.up_proj(x)                     # é‡åŒ–Linear: [B, N, 96] -> [B, N, 384]
        x_lora = self.up_lora_b(                          # LoRAè·¯å¾„
            self.up_lora_a(x)                             # [B, N, 96] -> [B, N, 16]
        ) * (self.lora_alpha / self.lora_r)               # [B, N, 16] -> [B, N, 384] * (32/16)
        x = x_quantized + x_lora                         # é‡åŒ–ç»“æœ + LoRAç»“æœ

        # === æ®‹å·®è¿æ¥ + ç¼©æ”¾ ===
        return x * self.gamma + inputs
```

### 2. å‚æ•°é‡å¯¹æ¯”

#### åŸå§‹Adapterå‚æ•°é‡
```
Down Projection: 384 Ã— 96 = 36,864
Up Projection:   96 Ã— 384 = 36,864
Temporal Conv:   ~3,456
Total:           ~76,000 å‚æ•°/å±‚
```

#### QLoRA Adapterå‚æ•°é‡
```
é‡åŒ–Linear (4-bit):      ~0 (é‡åŒ–å­˜å‚¨ï¼Œä¸è®¡å…¥å¯è®­ç»ƒå‚æ•°)
LoRA Down A:            384 Ã— 16 = 6,144
LoRA Down B:            16 Ã— 96 = 1,536
LoRA Up A:              96 Ã— 16 = 1,536
LoRA Up B:              16 Ã— 384 = 6,144
Temporal Conv:          ~3,456
Gamma:                 1
Total:                  ~18,800 å‚æ•°/å±‚ (å‡å°‘75%)
```

---

## ğŸŒ ç½‘ç»œç»“æ„è¯¦è§£

### 1. VisionTransformerQLoRAæ•´ä½“ç»“æ„

```python
class VisionTransformerQLoRA:
    """
    è¾“å…¥: è§†é¢‘å¸§ [B, C, T, H, W]
    è¾“å‡º: ç‰¹å¾å›¾ [B, C, T, H', W'] æˆ– ç‰¹å¾å‘é‡ [B, C]
    """

    def __init__(self):
        # 1. Patch Embedding
        self.patch_embed = PatchEmbed(...)  # [B, C, T, H, W] -> [B, N, C]

        # 2. Positional Embedding
        self.pos_embed = SinusoidEncoding(...)

        # 3. Transformer Blocks (12å±‚)
        self.blocks = ModuleList([
            QLoRABlock(
                embed_dims=384,
                num_heads=6,
                use_adapter=True,  # æ‰€æœ‰12å±‚éƒ½ä½¿ç”¨Adapter
                ...
            ) for i in range(12)
        ])

        # 4. Normalization
        self.norm = LayerNorm(...)

    def forward(self, x):
        # Patch Embedding
        x = self.patch_embed(x)  # [B, C, T, H, W] -> [B, N, C]

        # Positional Embedding
        x = x + self.pos_embed

        # Transformer Blocks
        for block in self.blocks:
            x = block(x, h, w)  # æ¯å±‚éƒ½ç»è¿‡QLoRA Adapter

        # Normalization
        x = self.norm(x)

        return x  # è¿”å›ç‰¹å¾å›¾æˆ–ç‰¹å¾å‘é‡
```

### 2. QLoRABlockç»“æ„

```python
class QLoRABlock:
    """
    æ¯ä¸ªBlockåŒ…å«ï¼š
    1. Self-Attention (å†»ç»“)
    2. MLP (å†»ç»“)
    3. QLoRA Adapter (å¯è®­ç»ƒ)
    """

    def forward(self, x, h, w):
        # Self-Attention (å†»ç»“)
        x = x + self.drop_path(
            self.attn(self.norm1(x))
        )

        # MLP (å†»ç»“)
        x = x + self.drop_path(
            self.mlp(self.norm2(x))
        )

        # QLoRA Adapter (å¯è®­ç»ƒ) â­
        if self.use_adapter:
            x = self.adapter(x, h, w)

        return x
```

### 3. æ•°æ®æµå›¾

```
è¾“å…¥è§†é¢‘: [B, 3, 768, 160, 160]
    â”‚
    â”œâ”€> Chunkåˆ†å‰²: [B*48, 3, 16, 160, 160]  (768å¸§åˆ†æˆ48ä¸ªchunkï¼Œæ¯ä¸ª16å¸§)
    â”‚
    â”œâ”€> Patch Embedding: [B*48, 800, 384]   (16å¸§ Ã— 10Ã—10 patches = 1600, å®é™…800)
    â”‚
    â”œâ”€> Position Embedding: [B*48, 800, 384]
    â”‚
    â”œâ”€> Block 0-11 (æ¯ä¸ªBlock):
    â”‚   â”‚
    â”‚   â”œâ”€> Self-Attention (å†»ç»“): [B*48, 800, 384]
    â”‚   â”‚
    â”‚   â”œâ”€> MLP (å†»ç»“): [B*48, 800, 384]
    â”‚   â”‚
    â”‚   â””â”€> QLoRA Adapter (å¯è®­ç»ƒ): [B*48, 800, 384]
    â”‚       â”‚
    â”‚       â”œâ”€> Down: 384 -> 96 (é‡åŒ– + LoRA)
    â”‚       â”œâ”€> Temporal Conv: æ—¶åºå»ºæ¨¡
    â”‚       â””â”€> Up: 96 -> 384 (é‡åŒ– + LoRA)
    â”‚
    â”œâ”€> Post-processing:
    â”‚   â”œâ”€> Reduce: [B*48, 800, 384] -> [B*48, 384]
    â”‚   â”œâ”€> Rearrange: [B*48, 384] -> [B, 384, 768]
    â”‚   â””â”€> Interpolate: [B, 384, 768] (å¯¹é½åˆ°window_size)
    â”‚
    â””â”€> è¾“å‡ºç‰¹å¾: [B, 384, 768]
```

---

## ğŸ¯ é‡åŒ–ä¸LoRAæœºåˆ¶

### 1. é‡åŒ–æœºåˆ¶ (4-bit Quantization)

```python
# åŸå§‹Linearå±‚
self.down_proj = nn.Linear(384, 96)  # 36,864 å‚æ•° (FP32)

# é‡åŒ–Linearå±‚ (bitsandbytes)
self.down_proj = bnb.nn.Linear4bit(384, 96)  # ~4,608 å‚æ•° (4-bit)
# æ˜¾å­˜èŠ‚çœ: 36,864 Ã— 32bit â†’ 36,864 Ã— 4bit = 75% æ˜¾å­˜èŠ‚çœ
```

**é‡åŒ–åŸç†ï¼š**
- ä½¿ç”¨ `bitsandbytes` åº“çš„ `Linear4bit`
- æƒé‡è¢«é‡åŒ–ä¸º4-bitæ•´æ•°ï¼ŒåŠ¨æ€é‡åŒ–èŒƒå›´
- å‰å‘ä¼ æ’­æ—¶è‡ªåŠ¨åé‡åŒ–å›FP16è¿›è¡Œè®¡ç®—
- åå‘ä¼ æ’­æ—¶åªæ›´æ–°é‡åŒ–å‚æ•°ï¼ˆabsmax, quant_stateç­‰ï¼‰

### 2. LoRAæœºåˆ¶ (Low-Rank Adaptation)

```python
# åŸå§‹æŠ•å½±: W Ã— x
# å‚æ•°é‡: 384 Ã— 96 = 36,864

# LoRAåˆ†è§£: W + Î”W = W + B Ã— A
# å…¶ä¸­:
#   A: [384, 16]  (6,144 å‚æ•°)
#   B: [16, 96]   (1,536 å‚æ•°)
#   æ€»å‚æ•°é‡: 7,680 (å‡å°‘79%)

# å‰å‘ä¼ æ’­:
x_lora = B(A(x)) * (alpha / r)
# alpha = 32, r = 16, ç¼©æ”¾å› å­ = 2.0
```

**LoRAåŸç†ï¼š**
- å‡è®¾æƒé‡æ›´æ–° Î”W æ˜¯ä½ç§©çš„
- å°† Î”W åˆ†è§£ä¸ºä¸¤ä¸ªå°çŸ©é˜µçš„ä¹˜ç§¯: Î”W = B Ã— A
- åªè®­ç»ƒ A å’Œ Bï¼ŒåŸå§‹æƒé‡ W å†»ç»“
- é€šè¿‡ç¼©æ”¾å› å­ Î±/r æ§åˆ¶LoRAçš„è´¡çŒ®

### 3. é‡åŒ– + LoRA ç»„åˆ

```python
# å‰å‘ä¼ æ’­æµç¨‹:
x_quantized = quantized_linear(x)      # é‡åŒ–Linear: æ˜¾å­˜é«˜æ•ˆ
x_lora = lora_b(lora_a(x)) * scale     # LoRA: å‚æ•°é«˜æ•ˆ
x = x_quantized + x_lora               # ä¸¤è€…ç»“åˆ

# ä¼˜åŠ¿:
# 1. é‡åŒ–Linear: å‡å°‘75%æ˜¾å­˜å ç”¨
# 2. LoRA: å‡å°‘79%å¯è®­ç»ƒå‚æ•°
# 3. ç»„åˆ: æ—¢èŠ‚çœæ˜¾å­˜åˆå‡å°‘å‚æ•°ï¼ŒåŒæ—¶ä¿æŒæ€§èƒ½
```

---

## ğŸ“ è®­ç»ƒç­–ç•¥

### 1. å‚æ•°å†»ç»“ç­–ç•¥

```python
def _freeze_layers(self):
    """åªè®­ç»ƒAdapterå’ŒLoRAå‚æ•°ï¼Œå…¶ä»–å…¨éƒ¨å†»ç»“"""

    # å†»ç»“Patch Embedding
    self.patch_embed.eval()
    for param in self.patch_embed.parameters():
        param.requires_grad = False

    # å†»ç»“Blocksä¸­çš„Attentionå’ŒMLP
    for block in self.blocks:
        for name, module in block.named_children():
            if "adapter" not in name and "lora" not in name:
                module.eval()
                for param in module.parameters():
                    param.requires_grad = False

    # åªè®­ç»ƒAdapterå’ŒLoRAå‚æ•°
    # æ³¨æ„: é‡åŒ–Linearçš„å‚æ•°ä¸èƒ½è®¾ç½®requires_grad
    # åªè®¾ç½®LoRAå±‚ã€gammaã€temporal convç­‰éé‡åŒ–å‚æ•°
```

### 2. ä¼˜åŒ–å™¨é…ç½®

```python
optimizer = dict(
    type="AdamW",
    lr=1e-4,
    paramwise=True,
    backbone=dict(
        lr=0,  # ä¸»backboneå­¦ä¹ ç‡ä¸º0ï¼ˆå†»ç»“ï¼‰
        custom=[
            dict(name="adapter", lr=2e-4, weight_decay=0.05),  # Adapterå­¦ä¹ ç‡
            dict(name="lora", lr=2e-4, weight_decay=0.05),     # LoRAå­¦ä¹ ç‡
        ],
        exclude=["backbone"],  # æ’é™¤ä¸»backbone
    ),
)
```

### 3. å¯è®­ç»ƒå‚æ•°ç»Ÿè®¡

```python
# åœ¨åˆå§‹åŒ–æ—¶æ‰“å°å‚æ•°ç»Ÿè®¡
num_vit_param = sum(p.numel() for name, p in self.named_parameters()
                     if "adapter" not in name and "lora" not in name)
num_adapter_param = sum(p.numel() for name, p in self.named_parameters()
                        if "adapter" in name or "lora" in name)
ratio = num_adapter_param / num_vit_param * 100

# è¾“å‡ºç¤ºä¾‹:
# QLoRA - ViT's param: 22,000,000, QLoRA Adapter's params: 225,600, ratio: 1.0%
```

---

## ğŸ“Š å…³é”®è®¾è®¡å†³ç­–

### 1. ä¸ºä»€ä¹ˆä¿ç•™Temporal Convolutionï¼Ÿ

- **æ—¶åºå»ºæ¨¡èƒ½åŠ›**: Temporal Convä¸“é—¨å¤„ç†è§†é¢‘çš„æ—¶åºä¿¡æ¯
- **ä¸é‡åŒ–**: Convå±‚å‚æ•°é‡å°ï¼Œé‡åŒ–æ”¶ç›Šæœ‰é™
- **ä¿æŒæ€§èƒ½**: å®Œæ•´AdapteråŒ…å«Temporal Convï¼Œæ€§èƒ½æ›´å¥½

### 2. ä¸ºä»€ä¹ˆä½¿ç”¨4-bité‡åŒ–ï¼Ÿ

- **æ˜¾å­˜èŠ‚çœ**: 4-bitç›¸æ¯”FP32èŠ‚çœ87.5%æ˜¾å­˜
- **æ€§èƒ½å¹³è¡¡**: 4-bitåœ¨æ€§èƒ½å’Œæ˜¾å­˜ä¹‹é—´å–å¾—å¹³è¡¡
- **bitsandbytesæ”¯æŒ**: æˆç†Ÿçš„4-bité‡åŒ–å®ç°

### 3. ä¸ºä»€ä¹ˆLoRA rankè®¾ä¸º16ï¼Ÿ

- **å‚æ•°æ•ˆç‡**: rank=16æ—¶å‚æ•°é‡çº¦ä¸ºåŸå§‹çš„1/24
- **æ€§èƒ½ä¿æŒ**: å®éªŒè¡¨æ˜rank=16èƒ½ä¿æŒè¾ƒå¥½æ€§èƒ½
- **å¯è°ƒèŠ‚**: å¯é€šè¿‡`lora_r`å‚æ•°è°ƒæ•´

### 4. ä¸ºä»€ä¹ˆæ‰€æœ‰12å±‚éƒ½ä½¿ç”¨Adapterï¼Ÿ

- **å…¨é¢å¾®è°ƒ**: æ‰€æœ‰å±‚éƒ½å‚ä¸é€‚åº”ï¼Œæ•ˆæœæ›´å¥½
- **å‚æ•°å¯æ§**: QLoRAä½¿æ€»å‚æ•°é‡ä»ç„¶å¾ˆå°
- **é…ç½®çµæ´»**: å¯é€šè¿‡`adapter_index`é€‰æ‹©ç‰¹å®šå±‚

---

## ğŸ” ä»£ç å…³é”®ç‚¹

### 1. é‡åŒ–çŠ¶æ€è¿‡æ»¤ï¼ˆEMAå…¼å®¹ï¼‰

```python
def filter_quantization_state(state_dict):
    """è¿‡æ»¤æ‰é‡åŒ–å±‚çš„é¢å¤–çŠ¶æ€ä¿¡æ¯"""
    filtered_dict = {}
    for key, value in state_dict.items():
        if "absmax" in key or "quant_map" in key or "quant_state" in key:
            continue  # è·³è¿‡é‡åŒ–å…ƒæ•°æ®
        filtered_dict[key] = value
    return filtered_dict
```

### 2. æ¢¯åº¦è®¾ç½®ï¼ˆé‡åŒ–å±‚å…¼å®¹ï¼‰

```python
# é‡åŒ–Linearçš„å‚æ•°ä¸èƒ½è®¾ç½®requires_grad
# åªè®¾ç½®LoRAå±‚å’Œgammaç­‰éé‡åŒ–å‚æ•°
for name, param in n.named_parameters():
    if "down_proj" in name or "up_proj" in name:
        continue  # è·³è¿‡é‡åŒ–Linear
    if param.dtype in [torch.float32, torch.float16, torch.bfloat16]:
        param.requires_grad = True
```

---

## ğŸ“ˆ æ€§èƒ½å¯¹æ¯”

| æ–¹æ³• | å¯è®­ç»ƒå‚æ•° | æ˜¾å­˜å ç”¨ | è®­ç»ƒé€Ÿåº¦ | mAP |
|------|-----------|---------|---------|-----|
| å…¨é‡å¾®è°ƒ | 22M | 100% | åŸºå‡† | åŸºå‡† |
| Adapter | 225K (1.0%) | ~80% | å¿« | ~98% |
| LoRA | 180K (0.8%) | ~85% | å¿« | ~97% |
| **QLoRA** | **225K (1.0%)** | **~60%** | **å¿«** | **~98%** |

---

## ğŸ¯ æ€»ç»“

QLoRAä¸AdaTADçš„ç»“åˆå®ç°äº†ï¼š
1. **å‚æ•°é«˜æ•ˆ**: åªè®­ç»ƒ1%çš„å‚æ•°
2. **æ˜¾å­˜é«˜æ•ˆ**: èŠ‚çœ40%æ˜¾å­˜
3. **æ€§èƒ½ä¿æŒ**: è¾¾åˆ°å…¨é‡å¾®è°ƒ98%çš„æ€§èƒ½
4. **æ˜“äºéƒ¨ç½²**: é‡åŒ–æƒé‡ä¾¿äºéƒ¨ç½²

æ ¸å¿ƒåˆ›æ–°ç‚¹ï¼š
- é‡åŒ–Linearå±‚å‡å°‘æ˜¾å­˜
- LoRAå±‚å‡å°‘å‚æ•°
- ä¿ç•™Temporal Convolutionä¿æŒæ€§èƒ½
- çµæ´»çš„å†»ç»“ç­–ç•¥

